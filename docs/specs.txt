=======================
Benchmark specification
=======================

Notes attempting to detangle the benchmarking process.

Application configuration parameters:
   - Algorithm/calculation (major subroutines)
   - Total FLOP count
      - FLOP count for major subroutines
   - Total memory
      - Memory per rank (as a function of rank count)
   - MPI message count, distribution of message sizes

Application benchmark metrics:
   - Runtime
   - FLOP/sec
   - Memory bandwidth (GB/sec)
      - Interconnect
      - RAM <-> CPU
      - Cache (if applicable)

Metric contexts:
   - Total time
   - Initialisation, Main loop, Finalise
   - Major subroutines (i.e. top 5 functions as relative runtime)
   - Scalability over multiple CPU counts

Process:
- Identify current metric needs
- Evaluate capacity of available hardware


Ocean modelling
---------------

Ocean model simulations comprise some of the largest simulations on Raijin,
potentially using over 10,000 CPUs per simulation.  Past and current
simulations are summarised below.

==========  =================    =========   =========
Resolution  Grid size            Scaling     Operation
==========  =================    =========   =========
1°            360 x  300 x 50         *          96
0.25°        1440 x 1080 x 75        4480      1920
0.1°         3600 x 2700 x 75       20000      5136
0.03°       12000 x 9000 x ??      100000       ??
==========  =================    ==========  =========

MOM 5 has proven to be a highly scaleable model with few bottlenecks in
production simulations.

0.25° metrics
+++++++++++++

Total timestep time: ~166s
Total FLOPs: ??
Total messages:

Total byte send/recv per timestep:
   Interior:   4827120 / 4827120
   South:      3430328 / 3448728
   Northwest:  4964024 / 4827120
   NW (end):   4961736 / 4827120
   Northeast:  4835376 / 4935480
   NE (end):   4836520 / 4934336

Naive bandwidth: ~28 kiB/s but this is not accurate

----

Go down to biharmonic solver, interior rank.

Subroutine: ``ocean_bihgen_friction_mod.bihgen_friction_``
Bytes per step:
   - 1195200 bytes
   = 18 * 50 * (4*1 + 2*36 + 2*45) * 8 bytes
   = (18 halo updates) * (50 levels) * (4 corners + 2 EW + 2NS edges) * 8 bytes

BUT: Some halo updates are gathered in buffers.  In this case, only three are
sent:

   1. 8x ``stress`` (L1132-1139)
   2. 2x ``tmplap`` (L1182-1183)
   3. 8x ``stress`` (L1132-1139)

Barriers (as ``mpp_sync_self``) occur after each list element.

(There are also other halo updates, but they are conditionally called and do
not appear in the profile.)

Each bundle does 8 updates: 2 NS, 2EW, 4 corners.  Sizes:

The stress message sizes are:

   North-South:   144000 bytes
   East-West:     115200 bytes
   Corner:        3200 bytes

The Laplacian message sizes are:

   North-South:   36000 bytes
   East-West:     28800 bytes
   Corner:        800 bytes

The median empirical bandwidths from nine OSU tests are shown below.
Taking the fastest times and (t = bytes / bandwidth):

========    =========   ===========
Msg (B)     BW (MB/s)   Time (usec) 
========    =========   ===========
  3200      2965.15      1.1
115200      2882.26     40.0
144000      1517.31     94.9
  800        959.72      0.8
 28800      5300.15      5.4
 36000      5124.02      7.0
========    =========   ===========

The slowest operation is north-south transfer of the stress tensor.  So we
estimate the runtime as twice this operation:

   t ~= 2*95 + 7 sec
      = 197 usec

For 480 steps, this is about 0.1 seconds.  But we see much larger:

   (With Score-P overhead...)
   Time: 1.2 - 5s (70%: 1.8 - 3.2, 2.5 mean)
   Time per step: 5ms

So not sure what's going on...

(What is this???)
Mean message size: 49800 bytes
   - Actual is probably spread out: work it out?
Mean bandwidth: ~0.2 GiB/s
Peak cross-node: ~5 GiB/s
   - TODO: Latency??

TODO:
- Total FLOPs, est. FLOP/sec (should be ~1GF/sec)
   - Little vectorization speedup
   - RAM-bound?

- Communication
   - # of messages
   - Bandwidth (bytes/sec)
      (Likely not comm bound?)

- Other??


TODO: MOM6
