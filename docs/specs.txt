=======================
Benchmark specification
=======================

Notes attempting to detangle the benchmarking process.

Application configuration parameters:
   - Algorithm/calculation (major subroutines)
   - Total FLOP count
      - FLOP count for major subroutines
   - Total memory
      - Memory per rank (as a function of rank count)
   - MPI message count, distribution of message sizes

Application benchmark metrics:
   - Runtime
   - FLOP/sec
   - Memory bandwidth (GB/sec)
      - Interconnect
      - RAM <-> CPU
      - Cache (if applicable)

Metric contexts:
   - Total time
   - Initialisation, Main loop, Finalise
   - Major subroutines (i.e. top 5 functions as relative runtime)
   - Scalability over multiple CPU counts

Process:
- Identify current metric needs
- Evaluate capacity of available hardware


Ocean modelling
---------------

Ocean model simulations comprise some of the largest simulations on Raijin,
potentially using over 10,000 CPUs per simulation.  Current and future
configurations are summarised below.

==========  ==================   =========   =========
Resolution  Grid size            Peak        Operation
==========  ==================   =========   =========
1°            360 x  300 x  50         *            96
0.25°        1440 x 1080 x  75        4480        1920
0.1°         3600 x 2700 x  75       20000        5136
0.03°       12000 x 9000 x 140      100000         *
==========  ==================   =========   =========

MOM 5 has proven to be a highly scaleable model with few major bottlenecks in
production simulations.

In the analysis below, we include the total main loop time and the time in the
ocean submodel, along with four ocean subregions which comprise the largest
shares of runtime:

* *MDPPM*, the finite volume tracer advection scheme
* *Bih friction*, the biharmonic friction solver
* *Barotropic*, the two-dimensional free surface solver
* *KPP*, the vertical mixing scheme.

This includes representative subroutines of 2D, 3D, and vertical physics
operations within the model.


Calculation
+++++++++++

The total FLOPs over a 1-day simulation as reported by PAPI are shown below.
Runtimes are rescaled from a 10-day simulation without profiling and used to
compute an effective performance rate in GFLOPs per second.

============   ======   =======  ============
Region         TFLOPs   Runtime  GFLOP/core/s
============   ======   =======  ============
Main Loop        18.9      19.7          1.00
Ocean            18.0      16.5          1.13
MDPPM             7.5       2.5          3.13
Bih friction      1.4       1.6          0.91
Barotropic        1.7       1.0          1.77
KPP               1.2       1.4          0.89
============   ======   =======  ============

MOM 5 performance is about ten times lower than the theoretical peak of
approximately 12 GFLOP/sec, with the majority of the model running at about 1
GFLOP/sec.  The notable exception is the finite volume MDPPM advection, whose
performance is three times higher than the rest of the model.

It suggests that the majority of the model is RAM-bound, although the MDPPM
calculation may be bound at an internal cache level.


Vectorisation
+++++++++++++

The MOM 5 source produces vectorised code to a high degree.  This is confirmed
by the ratio of vectorized FLOPs to total FLOPs, as reported by PAPI
(``PAPI_DP_OPS`` and ``PAPI_VEC_DP``) and shown in the table below.

============   ======   ====== 
Region         TFLOPs   % SIMD
============   ======   ====== 
Main Loop        18.9     90.5
Ocean            18.0     90.5
MDPPM             7.5     94.1
Bih friction      1.4     90.7
Barotropic        1.7     95.9
KPP               1.2     78.3
============   ======   ====== 

The numbers confirm that SIMD vectorisation instructions are being used,
although there is no guarantee of their effective use.

Runtimes for the major subroutines of the model are shown below across
different architectures and vectorisation instructions enabled.  We use a
10-day simulation of the 0.25° global MOM-SIS configuration.  The median
runtime is reported over an ensemble of runs.  Ensemble size varies across
configurations, from 6 to 10 runs.

In the table below, "SB" denotes Sandy Bridge, "BW" is Broadwell, and "KL" is
Knights Landing.

============   ===========    ========    ========    =========   =========
Region         Serial (SB)    AVX (SB)    AVX (BW)    AVX2 (BW)   AVX2 (KL)
============   ===========    ========    ========    =========   =========
Main Loop            226.8       197.2       179.9        172.7       491.6
Ocean                179.3       165.0       160.6        156.7       382.6
MDPPM                 31.4        25.1        22.7         21.0        61.1
Bih friction          16.7        15.9        16.0         15.8        33.1
Barotropic            11.2         9.6         7.3          6.7        40.6
KPP                   14.7        14.0        13.6         13.6        29.2
============   ===========    ========    ========    =========   =========

The table indicates that vectorization is moderately beneficial to MOM.  AVX
vectorisation introduces a 1.15x speedup of the MOM-SIS run and 1.09x speedup
of the ocean core.  The MDPPM tracer advection appears to benefits the most
from vectorisation, showing a 1.25x speedup.  The barotropic solver shows a
similar improvement of 1.17x AVX speedup.  The other regions show minor
speedups.

Execution on the Broadwell architecture yields addition speedups of 1.10x of
the main loop, but only 1.03x speedup of the ocean core.  MDPPM tracer
advection and the barotropic solver are again significant, with speedups of
1.10x and 1.32x, respectively, though again the biharmonic friction and KPP
vertical mixing speedups are modest.  Activation of AVX2 shows additional
improvements, but the speedups are more modest, on the order of 1.02x.

MOM5 currently does not run using AVX-512 vectorisation and produces
segmentation faults which have not yet been investigates.  But the results in
this section show that we should only expect at best a modest improvement in
runtime.


Communication
+++++++++++++

Total timestep time: ~166s
Total FLOPs: ??
Total messages:

Total byte send/recv per timestep:
   Interior:   4827120 / 4827120
   South:      3430328 / 3448728
   Northwest:  4964024 / 4827120
   NW (end):   4961736 / 4827120
   Northeast:  4835376 / 4935480
   NE (end):   4836520 / 4934336

Naive bandwidth: ~28 kiB/s but this is not accurate

----

Go down to biharmonic viscosity, interior rank.

Subroutine: ``ocean_bihgen_friction_mod.bihgen_friction_``
Bytes per step:
   - 1195200 bytes
   = 18 * 50 * (4*1 + 2*36 + 2*45) * 8 bytes
   = (18 halo updates) * (50 levels) * (4 corners + 2 EW + 2NS edges) * 8 bytes

BUT: Some halo updates are gathered in buffers.  In this case, only three are
sent:

   1. 8x ``stress`` (L1132-1139)
   2. 2x ``tmplap`` (L1182-1183)
   3. 8x ``stress`` (L1132-1139)

Barriers (as ``mpp_sync_self``) occur after each list element.

(There are also other halo updates, but they are conditionally called and do
not appear in the profile.)

Each bundle does 8 updates: 2 NS, 2EW, 4 corners.  Sizes:

The stress message sizes are:

   North-South:   144000 bytes
   East-West:     115200 bytes
   Corner:        3200 bytes

The Laplacian message sizes are:

   North-South:   36000 bytes
   East-West:     28800 bytes
   Corner:        800 bytes

The median empirical bandwidths from nine OSU tests are shown below.
Taking the fastest times and (t = bytes / bandwidth):

========    =========   ===========
Msg (B)     BW (MB/s)   Time (usec) 
========    =========   ===========
    3200      2965.15           1.1
  115200      2882.26          40.0
  144000      1517.31          94.9
     800       959.72           0.8
   28800      5300.15           5.4
   36000      5124.02           7.0
========    =========   ===========

The slowest operation is north-south transfer of the stress tensor.  So we
estimate the runtime as twice this operation:

   t ~= 2*95 + 7 sec
      = 197 usec

For 480 steps, this is about 0.1 seconds.  But we see much larger:

   (With Score-P overhead...)
   Time: 1.2 - 5s (70%: 1.8 - 3.2, 2.5 mean)
   Time per step: 5ms

So not sure what's going on...

(What is this???)
Mean message size: 49800 bytes
   - Actual is probably spread out: work it out?
Mean bandwidth: ~0.2 GiB/s
Peak cross-node: ~5 GiB/s
   - TODO: Latency??

TODO:
- Total FLOPs, est. FLOP/sec (should be ~1GF/sec)
   - Little vectorization speedup
   - RAM-bound?

- Communication
   - # of messages
   - Bandwidth (bytes/sec)
      (Likely not comm bound?)

- Other??


TODO: MOM6
