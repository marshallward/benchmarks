=======================
Benchmark specification
=======================

Notes attempting to detangle the benchmarking process.

Application configuration parameters:
   - Algorithm/calculation (major subroutines)
   - Total FLOP count
      - FLOP count for major subroutines
   - Total memory
      - Memory per rank (as a function of rank count)
   - MPI message count, distribution of message sizes

Application benchmark metrics:
   - Runtime
   - FLOP/sec
   - Memory bandwidth (GB/sec)
      - Interconnect
      - RAM <-> CPU
      - Cache (if applicable)

Metric contexts:
   - Total time
   - Initialisation, Main loop, Finalise
   - Major subroutines (i.e. top 5 functions as relative runtime)
   - Scalability over multiple CPU counts

Process:
- Identify current metric needs
- Evaluate capacity of available hardware


Ocean modelling
---------------

Ocean model simulations comprise some of the largest simulations on Raijin,
potentially using over 10,000 CPUs per simulation.  Past and current
simulations are summarised below.

==========  =================    =========   =========
Resolution  Grid size            Scaling     Operation
==========  =================    =========   =========
1°            360 x  300 x 50         *          96
0.25°        1440 x 1080 x 75        4480      1920
0.1°         3600 x 2700 x 75       20000      5136
0.03°       12000 x 9000 x ??      100000       ??
==========  =================    ==========  =========

MOM 5 has proven to be a highly scaleable model with few bottlenecks in
production simulations.

0.25° metrics
+++++++++++++

Total timestep time: ~166s
Total FLOPs: ??
Total messages:

Total byte send/recv per timestep:
   Interior:   4827120 / 4827120
   South:      3430328 / 3448728
   Northwest:  4964024 / 4827120
   NW (end):   4961736 / 4827120
   Northeast:  4835376 / 4935480
   NE (end):   4836520 / 4934336

Naive bandwidth: ~28 kiB/s but this is not accurate

----

Go down to biharmonic solver, interior rank.

Subroutine: ``ocean_bihgen_friction_mod.bihgen_friction_``
Bytes per step:
   - 1195200 bytes
   = 18 * 50 * (4*1 + 2*36 + 2*45) * 8 bytes
   = (18 halo updates) * (50 levels) * (4 corners + 2 EW + 2NS edges) * 8 bytes

BUT: Some halo updates are gathered in buffers.  In this case, only three are
sent:

   1. 8x ``stress`` (L1132-1139)
   2. 2x ``tmplap`` (L1182-1183)
   3. 8x ``stress`` (L1132-1139)

(Plus two infrequent updates, unused?)

Each bundle does 8 updates: 2 NS, 2EW, 4 corners.  Sizes:

8 fields * 45 pts * 50 depth * 1 halo * 8 bytes

   NS: 144000 bytes
   EW: 115200 bytes
   Cr: 3200 bytes

The theoretical bandwidth and times from OSU::

    # OSU MPI Bandwidth Test v5.3
    # Size      Bandwidth (MB/s)
    3200                 2883.69
    115200               2585.94
    144000               1613.16
    800                   957.84
    28800                1312.54
    36000                5446.17

    # OSU MPI Bandwidth Test v5.3
    # Size      Bandwidth (MB/s)
    3200                 2786.77
    115200               3100.62
    144000               1114.06
    800                   970.61
    28800                3249.32
    36000                2413.08


(With Score-P overhead...)
Time: 1.2 - 5s (70%: 1.8 - 3.2, 2.5 mean)
Time per step: 5ms

Mean message size: 49800 bytes
   - Actual is probably spread out: work it out?
Mean bandwidth: ~0.2 GiB/s
Peak cross-node: ~5 GiB/s
   - TODO: Latency??

TODO:
- Total FLOPs, est. FLOP/sec (should be ~1GF/sec)
   - Little vectorization speedup
   - RAM-bound?

- Communication
   - # of messages
   - Bandwidth (bytes/sec)
      (Likely not comm bound?)

- Other??


TODO: MOM6
