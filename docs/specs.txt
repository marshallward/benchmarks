=======================
Benchmark specification
=======================

Notes attempting to detangle the benchmarking process.

Application configuration parameters:
   - Algorithm/calculation (major subroutines)
   - Total FLOP count
      - FLOP count for major subroutines
   - Total memory
      - Memory per rank (as a function of rank count)
   - MPI message count, distribution of message sizes

Application benchmark metrics:
   - Runtime
   - FLOP/sec
   - Memory bandwidth (GB/sec)
      - Interconnect
      - RAM <-> CPU
      - Cache (if applicable)

Metric contexts:
   - Total time
   - Initialisation, Main loop, Finalise
   - Major subroutines (i.e. top 5 functions as relative runtime)
   - Scalability over multiple CPU counts

Process:
- Identify current metric needs
- Evaluate capacity of available hardware


Ocean modelling
---------------

Ocean model simulations comprise some of the largest simulations on Raijin,
potentially using over 10,000 CPUs per simulation.  Current and future
configurations are summarised below.

==========  ==================   =========   =========
Resolution  Grid size            Peak        Operation
==========  ==================   =========   =========
1°            360 x  300 x  50         *            96
0.25°        1440 x 1080 x  75        4480        1920
0.1°         3600 x 2700 x  75       20000        5136
0.03°       12000 x 9000 x 140      100000         *
==========  ==================   =========   =========

MOM 5 has proven to be a highly scaleable model with few major bottlenecks in
production simulations.

In the analysis below, we include the total main loop time and the time in the
ocean submodel, along with four ocean subregions which comprise the largest
shares of runtime:

* *MDPPM*, the finite volume tracer advection scheme
* *Bih friction*, the biharmonic friction solver
* *Barotropic*, the two-dimensional free surface solver
* *KPP*, the vertical mixing scheme.

This includes representative subroutines of 2D, 3D, and vertical physics
operations within the model.


Calculation
+++++++++++

The total FLOPs over a 1-day simulation as reported by PAPI are shown below.
Runtimes are rescaled from a 10-day simulation without profiling and used to
compute an effective performance rate in GFLOPs per second.

============   ======   =======  ============
Region         TFLOPs   Runtime  GFLOP/core/s
============   ======   =======  ============
Main Loop        18.9      19.7          1.00
Ocean            18.0      16.5          1.13
MDPPM             7.5       2.5          3.13
Bih friction      1.4       1.6          0.91
Barotropic        1.7       1.0          1.77
KPP               1.2       1.4          0.89
============   ======   =======  ============

MOM 5 performance is about ten times lower than the theoretical peak of
approximately 12 GFLOP/sec, with the majority of the model running at about 1
GFLOP/sec.  The notable exception is the finite volume MDPPM advection, whose
performance is three times higher than the rest of the model.

It suggests that the majority of the model is RAM-bound, although the MDPPM
calculation may be bound at an internal cache level.


Vectorisation
+++++++++++++

The MOM 5 source produces vectorised code to a high degree.  This is confirmed
by the ratio of vectorized FLOPs to total FLOPs, as reported by PAPI
(``PAPI_DP_OPS`` and ``PAPI_VEC_DP``) and shown in the table below.

============   ======   ====== 
Region         TFLOPs   % SIMD
============   ======   ======
Main Loop        18.9     90.5
Ocean            18.0     90.5
MDPPM             7.5     94.1
Bih friction      1.4     90.7
Barotropic        1.7     95.9
KPP               1.2     78.3
============   ======   ====== 

The numbers confirm that SIMD vectorisation instructions are being used,
although there is no guarantee of their effective use.

Runtimes for the major subroutines of the model are shown below across
different architectures and vectorisation instructions enabled.  We use a
10-day simulation of the 0.25° global MOM-SIS configuration.  The median
runtime is reported over an ensemble of runs.  Ensemble size varies across
configurations, from 6 to 10 runs.

In the table below, "SB" denotes Sandy Bridge, "BW" is Broadwell, and "KL" is
Knights Landing.

============   ===========    ========    ========    =========   =========
Region         Serial (SB)    AVX (SB)    AVX (BW)    AVX2 (BW)   AVX2 (KL)
============   ===========    ========    ========    =========   =========
Main Loop            226.8       197.2       179.9        172.7       491.6
Ocean                179.3       165.0       160.6        156.7       382.6
MDPPM                 31.4        25.1        22.7         21.0        61.1
Bih friction          16.7        15.9        16.0         15.8        33.1
Barotropic            11.2         9.6         7.3          6.7        40.6
KPP                   14.7        14.0        13.6         13.6        29.2
============   ===========    ========    ========    =========   =========

The table indicates that vectorization is moderately beneficial to MOM.  AVX
vectorisation introduces a 1.15x speedup of the MOM-SIS run and 1.09x speedup
of the ocean core.  The MDPPM tracer advection appears to benefits the most
from vectorisation, showing a 1.25x speedup.  The barotropic solver shows a
similar improvement of 1.17x AVX speedup.  The other regions show minor
speedups.

Execution on the Broadwell architecture yields addition speedups of 1.10x of
the main loop, but only 1.03x speedup of the ocean core.  MDPPM tracer
advection and the barotropic solver are again significant, with speedups of
1.10x and 1.32x, respectively, though again the biharmonic friction and KPP
vertical mixing speedups are modest.  Activation of AVX2 shows additional
improvements, but the speedups are more modest, on the order of 1.02x.

MOM5 currently does not run using AVX-512 vectorisation and produces
segmentation faults which have not yet been investigated.  But the results in
this section show that we should only expect at best a modest improvement in
runtime.


Communication
+++++++++++++

Message passing in MOM 5 is dominated by halo updates, implemented using
unbuffered ``MPI_Isend`` calls, with halo data manually packed into arrays.
Most ranks send eight messages to nearest neighbors:  Two north/south packets,
two east/west packets, and four corner packets, with some modification along
boundaries and tripole ranks.

Collectives are only used during model initialisation and for other very
infrequent tasks, such as diagnostic and checksum calculations.  Nearly 99% of
communication time is devoted to halo exchange, and no collectives appear in
the timesteps of our configuration.

are only are applied at infrequent intervals, primarily related to
diagnostic and checksum calculations.  (Confirm)

For the results below, we examine the 0.25° global model using a 960-core
configuration (32 latitude ranks, 30 longitude ranks).

Total byte send/recv per timestep are shown below.

================  ======== ========
Rank type         Sent (B) Recv (B)
================  ======== ========
Interior tile      4827120  4827120
South boundary     3430328  3448728
NW tripole         4964024  4827120
NW tripole (end)   4961736  4827120
NE tripole         4835376  4935480
NE tripole (end)   4836520  4934336
================  ======== ========

The majority of ranks (93% in this case) consist of interior tiles.

The runtime per step is 0.35 seconds, and the approximate MPI time ratio within
the ocean timestep is 18%, so the broad bandwidth estimate ``B`` is

   B = 4827120 / (0.18 * 0.35) ~= 0.08 GB/s

This is less than 2% of peak performance (7 GB/s theoretical, 5-6 GB/s
empirical), although the estimation here ignores most of the detail in the
calculation.  We attempt to clarify this analysis by looking at the 2D and 3D
halo updates in individual sections.


Biharmonic viscosity
~~~~~~~~~~~~~~~~~~~~

The greatest portion of data transfer occurs within the biharmonic Smagorinsky
calculation (25% of total halo exchange data), which requires halo updates of
the many stress tensor components.

The biharmonic viscosity is calculated in the ``bihgen_friction`` function of
the ``ocean_bihgen_friction_mod`` module.  (See
``src/mom5/ocean_param/lateral/ocean_bihgen.F90``).

Each timestep requires an halo update of 18 3-dimensional fields, so that the
total number of bytes transferred by each interior rank is

   (18 halo updates) * (50 levels) * (4 corners + 2 EW + 2NS edges) * 8 bytes
   = 18 * 50 * (4*1 + 2*36 + 2*45) * 8 bytes
   = 1195200 bytes

However, the bandwidth is improved by gathering fields into larger message
buffers, so that only three messages are sent per timestep.  The messages are
sent in the order shown below.

   - 8 ``stress`` fields (L1132-1139)
   - 2 ``tmplap`` fields (L1182-1183)
   - 8 ``stress`` fields (L1244-1251)

Each halo exchanges requires 8 updates per field.

   - 2 north-south transfers
   - 2 east-west transfers
   - 4 corner transfers
     
The message sizes of the bundled ``stress`` fields are

   North-South:   144000 bytes
   East-West:     115200 bytes
   Corner:        3200 bytes

The bundled Laplacian message sizes are

   North-South:   36000 bytes
   East-West:     28800 bytes
   Corner:        800 bytes

We estimate the empirical bandwidths using the OSU point-to-point benchmark
tool.  The median transfer rate over nine tests, along with the estimated
transfer time, are shown below.

=============  =========   =========   ===========
Msg Type       Size (B)    BW (MB/s)   Time (μsec) 
=============  =========   =========   ===========
stress N/S        144000    1517.31*          94.9
stress E/W        115200    2882.26*          40.0
stress Corner       3200    2965.15            1.1
lap N/S            36000    5124.02            7.0
lap E/W            28800    5300.15            5.4
lap Corner           800     959.72            0.8
=============  =========   =========   ===========

Stars (*) indicate volatile bandwidth measurements, which can vary from under
1.5 GB/s to over 5 GB/s.

(WRONG!  How are messaged handled?  Are they concurrent?  Then bandwidth will
be lower.  Are they serial?  Then time would be more like 0.6 msec.  Then again

If we assume that the halo exchange is bounded by the slowest operation, then
all exchange is essentially throttled to 1.5 GB/s and the estimated halo
exchange time is 

   T_transfer ~= 2*95 μsec + 7 μsec
               = 0.2 msec.

For 480 steps, this is about 0.1 seconds.  But we see much larger:

   (With Score-P overhead...)
   Time: 1.2 - 5s (70%: 1.8 - 3.2, 2.5 mean)
   Time per step: 5ms

So not sure what's going on...


(What is this???)
Mean message size: 49800 bytes
   - Actual is probably spread out: work it out?
Mean bandwidth: ~0.2 GiB/s
Peak cross-node: ~5 GiB/s
   - TODO: Latency??

-------

Barotropic analysis: ``pred_corr_tropic_depth_bgrid``

Message split into vector and scalar updates.

Barotropic halo is 10, so the bytes per halo update are:
   (4 * 10 * 10) + (2 * 45 * 10) + (2 * 36 * 10)
   = 2020 bytes

Vector bytes / step: 581760
   = 72720 doubles
   = 36360 (x,y) pairs
   = 18 calls (17 bundled)

Scalar bytes / step: 323200
   = 40400 doubles
   = 20 calls (18 bundled)

There are 160 calls per main (baroclinic) timestep.
(Split is 80, why is it 2x?)

Updates:

1. Vector: ``udrho_bt``, ``forcing_bt`` bundled
2. Scalar: ``patm_bt``, ``steady_forcing``, ``eta_bt`` bundled
3. Scalar: ``thicku_bt``

4. 16x times:
   a. ``udrho_bt`` (L3490)
   b. ``eta_t_bt`` (L3529)

Message sizes and empirical bandwidths are shown below.

=============  ========    =========   ===========
Msg Type       Size (B)    BW (MB/s)   Time (usec) 
=============  ========    =========   ===========
scalar N/S         3600     3183.24          1.13
scalar E/W         2880     2736.33          1.05
scalar Corner       800      930.09          0.86
vector N/S         7200     1841.51*         3.91*
vector E/W         5760     3972.84          1.45
vector Corner      1600     1777.79          0.90
=============  ========    =========   ===========

(Star denotes volatile times)


=========   ========    =========   ===========
Msg Type    Size (B)    BW (MB/s)   Time (usec) 
=========   ========    =========   ===========
b Corner        2400
b E/W           8640
b N/S          10800
bv Corner       3200
bv E/W         11520
bv N/S         14400
=========   ========    =========   ===========

Estimated runtime per timestep:

   T = 16 * 4 us + 16 * 1.1 + (?) + (?)
     ~= 80 usec (<0.1 msec)

   Observed is ~11 msec
      (5.6 msec vector, 5.6 msec scalar)




TODO: MOM6
